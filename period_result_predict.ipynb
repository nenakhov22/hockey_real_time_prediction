{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14073515,"sourceType":"datasetVersion","datasetId":8958541},{"sourceId":14074333,"sourceType":"datasetVersion","datasetId":8959143}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom typing import Dict, List, Tuple\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nNUM_CONT_FEATS = 10 \n\ndef compute_normalization_stats(train_sequences, num_cont_feats=NUM_CONT_FEATS):\n    \"\"\"\n    train_sequences: список np.ndarray [T, F]\n    Считаем mean/std только по первым num_cont_feats признакам на всём трейн-сете.\n    \"\"\"\n    all_rows = []\n    for seq in train_sequences:\n        base = seq[:, :num_cont_feats]  # только continuous\n        all_rows.append(base.reshape(-1, num_cont_feats))\n    all_rows = np.concatenate(all_rows, axis=0)\n\n    mean = all_rows.mean(axis=0)\n    std = all_rows.std(axis=0)\n    std[std < 1e-6] = 1.0 \n    return mean, std\n\n\ndef apply_normalization(sequences, mean, std, num_cont_feats=NUM_CONT_FEATS):\n    \"\"\"\n    Применяем (x - mean) / std к первым num_cont_feats фичам.\n    Остальное (бинарные и one-hot) не трогаем.\n    \"\"\"\n    normed = []\n    for seq in sequences:\n        seq = seq.copy()\n        seq[:, :num_cont_feats] = (seq[:, :num_cont_feats] - mean) / std\n        normed.append(seq)\n    return normed\n\ndef build_encoders(pbp: pd.DataFrame) -> Dict[str, Dict[str, int]]:\n    \"\"\"\n    Строим один словарь для комбинированного типа события:\n    combined_type = EVENT_TYPE  или  EVENT_TYPE + SECONDARY_TYPE\n    (secondary_type с NaN не даёт отдельной категории)\n    \"\"\"\n    encoders: Dict[str, Dict[str, int]] = {}\n\n    et = (\n        pbp[\"event_type\"]\n        .astype(str)\n        .str.upper()\n    )\n\n    st = pbp[\"secondary_type\"].astype(\"string\")\n\n    combined = []\n    for e, s in zip(et, st):\n        if pd.isna(s) or s == \"\":\n            combined.append(e)                 \n        else:\n            combined.append(f\"{e}_{str(s).upper()}\") \n\n    combined_unique = sorted(set(combined))\n    encoders[\"event_type_combined\"] = {ct: i for i, ct in enumerate(combined_unique)}\n\n    return encoders\n\n# Построение выборок по минутам периода\n\ndef build_game_sequences(\n    pbp: pd.DataFrame,\n    encoders: Dict[str, Dict[str, int]],\n    min_events_per_game: int = 50,\n    window_size: int = 50,\n    max_period: int = 3,  # берём только 1–3 периоды\n) -> Tuple[List[np.ndarray], List[int], List[dict]]:\n    \"\"\"\n    Делаем выборки для обучения:\n\n    Каждая выборка = 50 последних событий перед минутной отметкой в периоде.\n    Лейбл = результат периода: 1, если home ведёт после конца периода, иначе 0.\n\n    Возвращает:\n      - X_list: список np.ndarray[window_size, F]\n      - y_list: список int (0/1)\n      - meta_list: список словарей с метой:\n            { \"game_id\": ..., \"period\": ..., \"minute_in_period\": ..., \"evant_time\": ... }\n    \"\"\"\n\n    required_cols = [\n        \"game_id\", \"event_type\", \"period\", \"game_seconds\",\n        \"home_score\", \"away_score\",\n        \"event_team_type\",\n        \"x_fixed\", \"y_fixed\",\n        \"shot_distance\", \"shot_angle\",\n        \"home_skaters\", \"away_skaters\",\n        \"extra_attacker\",\n        \"secondary_type\",\n    ]\n    missing = [c for c in required_cols if c not in pbp.columns]\n    if missing:\n        raise ValueError(f\"Missing required columns in pbp: {missing}\")\n\n    # сортируем внутри игры по времени\n    pbp_sorted = pbp.sort_values([\"game_id\", \"game_seconds\"]).reset_index(drop=True)\n\n    X_list: List[np.ndarray] = []\n    y_list: List[int] = []\n    meta_list: List[dict] = []\n\n    event_type_to_idx = encoders[\"event_type_combined\"]\n\n    for game_id, g in pbp_sorted.groupby(\"game_id\", sort=False):\n        if len(g) < max(min_events_per_game, window_size):\n            continue\n\n        # сброс индекса внутри игры\n        g = g.reset_index(drop=True)\n        N = len(g)\n\n        # общие массивы по игре\n        game_seconds = g[\"game_seconds\"].astype(float).values\n        max_t = max(game_seconds.max(), 1.0)\n        t_norm = game_seconds / max_t\n\n        period_arr = g[\"period\"].astype(int).values\n\n        home_score_all = g[\"home_score\"].astype(float).values\n        away_score_all = g[\"away_score\"].astype(float).values\n        score_diff_all = home_score_all - away_score_all\n\n        home_skaters_all = g[\"home_skaters\"].astype(float).fillna(0.0).values\n        away_skaters_all = g[\"away_skaters\"].astype(float).fillna(0.0).values\n\n        extra_attacker_all = g[\"extra_attacker\"].astype(float).fillna(0.0).values\n\n        is_home_event_all = g[\"event_team_type\"].eq(\"home\").fillna(False).astype(float).values\n        is_away_event_all = g[\"event_team_type\"].eq(\"away\").fillna(False).astype(float).values\n\n        x_fixed_all = g[\"x_fixed\"].astype(float).fillna(0.0).values\n        y_fixed_all = g[\"y_fixed\"].astype(float).fillna(0.0).values\n        shot_distance_all = g[\"shot_distance\"].astype(float).fillna(0.0).values\n        shot_angle_all = g[\"shot_angle\"].astype(float).fillna(0.0).values\n\n        cont_all = np.stack(\n            [\n                t_norm,\n                home_score_all,\n                away_score_all,\n                score_diff_all,\n                home_skaters_all,\n                away_skaters_all,\n                x_fixed_all,\n                y_fixed_all,\n                shot_distance_all,\n                shot_angle_all,\n            ],\n            axis=1,\n        ).astype(np.float32)\n\n        # бинарные фичи [N, 3]\n        binary_all = np.stack(\n            [\n                extra_attacker_all,\n                is_home_event_all,\n                is_away_event_all,\n            ],\n            axis=1,\n        ).astype(np.float32)\n\n        # period -> one-hot [N, max_period]\n        num_periods = max_period\n        period_ohe_all = np.zeros((N, num_periods), dtype=np.float32)\n        for i, p in enumerate(period_arr):\n            if 1 <= p <= num_periods:\n                period_ohe_all[i, p - 1] = 1.0\n        \n        et = g[\"event_type\"].astype(str).str.upper().values\n        st = g[\"secondary_type\"].astype(\"string\").values\n\n        combined = []\n        for e, s in zip(et, st):\n            if pd.isna(s) or s == \"\":\n                combined.append(e)\n            else:\n                combined.append(f\"{e}_{str(s).upper()}\")\n\n        num_types = len(event_type_to_idx)\n        type_ohe_all = np.zeros((N, num_types), dtype=np.float32)\n        for i, ct in enumerate(combined):\n            j = event_type_to_idx.get(ct)\n            if j is not None:\n                type_ohe_all[i, j] = 1.0\n\n        # по периодам\n        for p in range(1, max_period + 1):\n            period_mask = (period_arr == p)\n            if not period_mask.any():\n                continue\n\n            idxs_p = np.where(period_mask)[0]\n            times_p = game_seconds[idxs_p]\n\n            # считаем голы именно в этом периоде\n            first_idx = idxs_p[0]\n            last_idx  = idxs_p[-1]\n\n            # счёт перед стартом периода\n            if first_idx > 0:\n                home_before = home_score_all[first_idx - 1]\n                away_before = away_score_all[first_idx - 1]\n            else:\n                # для 1-го периода до начала игры счёт 0:0\n                home_before = 0.0\n                away_before = 0.0\n\n            # счёт после конца периода (накопительный)\n            home_end = home_score_all[last_idx]\n            away_end = away_score_all[last_idx]\n\n            # голы, забитые только в этом периоде\n            home_goals_period = home_end - home_before\n            away_goals_period = away_end - away_before\n\n            # таргет: кто выиграл период\n            if home_goals_period > away_goals_period:\n                home_win_period = 1\n            else:\n                # и ничьи, и победу гостей кодируем как 0\n                home_win_period = 0\n\n            # 20 минут в периоде → минутные границы\n            period_start = (p - 1) * 20 * 60  # 0, 1200, 2400\n            # minute_in_period = 1..20\n            for minute_in_period in range(1, 21):\n                threshold = period_start + 60 * minute_in_period\n\n                # ищем первое событие этого периода, у которого game_seconds >= threshold\n                pos_in_p = np.searchsorted(times_p, threshold, side=\"left\")\n                if pos_in_p >= len(times_p):\n                    # в этом периоде после этой минуты уже не было событий\n                    continue\n\n                idx_event = idxs_p[pos_in_p]  # индекс события в g (0..N-1)\n                if idx_event < window_size - 1:\n                    # недостаточно 50 предыдущих событий\n                    continue\n\n                # окно из 50 событий (включая событие на минутной границе)\n                start_idx = idx_event - window_size + 1\n                end_idx = idx_event + 1  # срез до end_idx не включительно\n                window_idx = np.arange(start_idx, end_idx)\n\n                cont = cont_all[window_idx]            # [T, 10]\n                binary = binary_all[window_idx]        # [T, 3]\n                period_ohe = period_ohe_all[window_idx]# [T, 3]\n                type_ohe = type_ohe_all[window_idx]    # [T, num_types]\n\n                feat_mat = np.concatenate(\n                    [cont, binary, period_ohe, type_ohe],\n                    axis=1\n                )  # [T, 10+3+3+num_types]\n\n                X_list.append(feat_mat)\n                y_list.append(home_win_period)\n                meta_list.append(\n                    {\n                        \"game_id\": game_id,\n                        \"period\": int(p),\n                        \"minute_in_period\": int(minute_in_period),\n                        \"event_time\": float(game_seconds[idx_event]),\n                    }\n                )\n\n    y_arr = np.array(y_list, dtype=float)\n    print(\"y NaN:\", np.isnan(y_arr).sum())\n    print(\"y уникальные значения:\", np.unique(y_arr))\n\n    return X_list, y_list, meta_list\n\n\n#  Dataset + collate_fn для PyTorch\n\nclass HockeySeqDataset(Dataset):\n    \"\"\"\n    Dataset: один объект = одна выборка (50 событий).\n    x: [T, F], y: 0/1 (home ведёт после периода), meta: словарь с метой.\n    \"\"\"\n    def __init__(self, sequences: List[np.ndarray], labels: List[int], metas: List[dict]):\n        assert len(sequences) == len(labels) == len(metas)\n        self.sequences = sequences\n        self.labels = labels\n        self.metas = metas\n\n    def __len__(self) -> int:\n        return len(self.sequences)\n\n    def __getitem__(self, idx: int):\n        x = torch.from_numpy(self.sequences[idx])  # [T, F]\n        y = torch.tensor(self.labels[idx], dtype=torch.long)\n        meta = self.metas[idx]\n        return x, y, meta\n\n\ndef collate_pad(batch):\n    \"\"\"\n    Пакуем батч в общий тензор [B, T_max, F] + маска [B, T_max]\n    и прокидываем meta как список словарей.\n    \"\"\"\n    xs, ys, metas = zip(*batch)  # добавили metas\n    lengths = [x.size(0) for x in xs]\n    F = xs[0].size(1)\n    B = len(xs)\n    T_max = max(lengths)\n\n    padded = xs[0].new_zeros((B, T_max, F))\n    mask = torch.zeros((B, T_max), dtype=torch.bool)  # True на реальных шагах\n\n    for i, x in enumerate(xs):\n        L = x.size(0)\n        padded[i, :L] = x\n        mask[i, :L] = True\n\n    y = torch.stack(ys)\n    metas = list(metas)  \n    return padded, y, mask, metas\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:59:36.580762Z","iopub.execute_input":"2025-12-11T18:59:36.581358Z","iopub.status.idle":"2025-12-11T18:59:41.850624Z","shell.execute_reply.started":"2025-12-11T18:59:36.581324Z","shell.execute_reply":"2025-12-11T18:59:41.849943Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class LSTMOutcomeModel(nn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int = 128, num_layers: int = 2, dropout: float = 0.2):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            input_dim,\n            hidden_dim,\n            num_layers=num_layers,\n            batch_first=True,\n            dropout=dropout if num_layers > 1 else 0.0,\n            bidirectional=False\n        )\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        # x: [B, T, F], mask: [B, T], True на реальных шагах\n        out, (h_n, c_n) = self.lstm(x)  # out: [B, T, H]\n        # берём скрытое состояние последнего шага по маске\n        last_indices = (mask.sum(dim=1) - 1).clamp(min=0)\n        batch_idx = torch.arange(x.size(0), device=x.device)\n        last_hidden = out[batch_idx, last_indices]  # [B, H]\n        logits = self.fc(self.dropout(last_hidden)).squeeze(-1)\n        return logits  # без сигмоиды, подадим в BCEWithLogitsLoss\n\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model, dtype=torch.float32)\n        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-np.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        if d_model % 2 == 1:\n            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n        else:\n            pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n        self.register_buffer('pe', pe)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # x: [B, T, d_model]\n        T = x.size(1)\n        x = x + self.pe[:, :T]\n        return x\n\n\nclass TransformerOutcomeModel(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        d_model: int = 128,\n        nhead: int = 4,\n        num_layers: int = 3,\n        dim_feedforward: int = 256,\n        dropout: float = 0.2\n    ):\n        super().__init__()\n        self.input_proj = nn.Linear(input_dim, d_model)\n        self.pos_encoder = PositionalEncoding(d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=nhead,\n            dim_feedforward=dim_feedforward,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(d_model, 1)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x: [B, T, F]\n        mask: [B, T]  (True = реальный шаг, False = паддинг)\n        \"\"\"\n        x = self.input_proj(x)      \n\n        x = self.pos_encoder(x)        \n\n        src_key_padding_mask = ~mask    \n\n        enc_out = self.encoder(\n            x,\n            src_key_padding_mask=src_key_padding_mask\n        )        \n\n        mask_f = mask.float()                       \n        enc_out_masked = enc_out * mask_f.unsqueeze(-1) \n\n        sum_vec = enc_out_masked.sum(dim=1)            \n        len_vec = mask_f.sum(dim=1).clamp(min=1.0)    \n        pooled = sum_vec / len_vec.unsqueeze(-1)    \n\n        logits = self.fc(self.dropout(pooled)).squeeze(-1)  \n        return logits    \n\n\n# 5. Обучение и валидация\n\ndef train_one_epoch(\n    model: nn.Module,\n    loader: DataLoader,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device\n) -> float:\n    model.train()\n    total_loss = 0.0\n    criterion = nn.BCEWithLogitsLoss()\n    for x, y, mask, metas in loader: \n        x = x.to(device)\n        y = y.float().to(device)\n        mask = mask.to(device)\n        optimizer.zero_grad()\n        logits = model(x, mask)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item() * x.size(0)\n    return total_loss / len(loader.dataset)\n\n\n@torch.no_grad()\ndef evaluate(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device\n) -> Tuple[float, float]:\n    \"\"\"\n    Возвращает (loss, accuracy) по всей валидации.\n    \"\"\"\n    model.eval()\n    criterion = nn.BCEWithLogitsLoss()\n    total_loss = 0.0\n    correct = 0\n    total = 0\n    for x, y, mask, metas in loader:  # metas игнорируем здесь\n        x = x.to(device)\n        y = y.float().to(device)\n        mask = mask.to(device)\n        logits = model(x, mask)\n        loss = criterion(logits, y)\n        total_loss += loss.item() * x.size(0)\n        probs = torch.sigmoid(logits)\n        preds = (probs >= 0.5).long()\n        correct += (preds.cpu() == y.long().cpu()).sum().item()\n        total += x.size(0)\n    return total_loss / len(loader.dataset), correct / max(total, 1)\n\n@torch.no_grad()\ndef evaluate_by_minute_and_period(\n    model: nn.Module,\n    loader: DataLoader,\n    device: torch.device\n) -> None:\n    \"\"\"\n    Считает accuracy на валидации отдельно для каждой (period, minute_in_period).\n    \"\"\"\n    model.eval()\n    bucket_stats = {}  # (period, minute) -> {\"correct\": int, \"total\": int}\n\n    for x, y, mask, metas in loader:\n        x = x.to(device)\n        y = y.float().to(device)\n        mask = mask.to(device)\n\n        logits = model(x, mask)\n        probs = torch.sigmoid(logits)\n        preds = (probs >= 0.5).long().cpu()\n        y_true = y.long().cpu()\n\n        correct_batch = (preds == y_true).numpy()\n\n        for is_corr, meta in zip(correct_batch, metas):\n            period = int(meta[\"period\"])\n            minute = int(meta[\"minute_in_period\"])\n            key = (period, minute)\n            if key not in bucket_stats:\n                bucket_stats[key] = {\"correct\": 0, \"total\": 0}\n            bucket_stats[key][\"correct\"] += int(is_corr)\n            bucket_stats[key][\"total\"] += 1\n\n    periods = sorted({p for (p, m) in bucket_stats.keys()})\n    for p in periods:\n        print(f\"\\n=== Period {p} ===\")\n        minutes = sorted(m for (pp, m) in bucket_stats.keys() if pp == p)\n        for m in minutes:\n            stats = bucket_stats[(p, m)]\n            acc = stats[\"correct\"] / stats[\"total\"]\n            print(f\"Minute {m:2d}: acc={acc:.3f} (n={stats['total']})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:59:41.851794Z","iopub.execute_input":"2025-12-11T18:59:41.852154Z","iopub.status.idle":"2025-12-11T18:59:41.873001Z","shell.execute_reply.started":"2025-12-11T18:59:41.852134Z","shell.execute_reply":"2025-12-11T18:59:41.872335Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"batch_size = 64\nnum_epochs = 20\nlr = 1e-3\nuse_transformer = True  # False -> LSTM\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\ndf1 = pd.read_csv(\"/kaggle/input/data-21-24/play_by_play_2021_22.csv\", encoding=\"latin1\")\ndf2 = pd.read_csv(\"/kaggle/input/data-21-24/play_by_play_2022_23.csv\", encoding=\"latin1\")\ndf3 = pd.read_csv(\"/kaggle/input/data-21-24/play_by_play_2023_24.csv\", encoding=\"latin1\")\ndf = pd.concat([df1, df2, df3], ignore_index=True)\n\n# убираем замены\ndf_model = df[df[\"event_type\"] != \"CHANGE\"].copy()\n\nencoders = build_encoders(df_model)\nX_list, y_list, meta_list = build_game_sequences(\n    df_model,\n    encoders,\n    min_events_per_game=50,\n    window_size=50,\n    max_period=3,\n)\n\n# meta_list содержит game_id\nall_game_ids = [m[\"game_id\"] for m in meta_list]\nunique_game_ids = sorted(set(all_game_ids))\n\n#80% матчей в train, 20% в val\nrng = np.random.default_rng(seed=42)\nrng.shuffle(unique_game_ids)\n\nn_games = len(unique_game_ids)\nn_train_games = int(0.8 * n_games)\ntrain_game_ids = set(unique_game_ids[:n_train_games])\nval_game_ids   = set(unique_game_ids[n_train_games:])\n\ntrain_idx = [i for i, m in enumerate(meta_list) if m[\"game_id\"] in train_game_ids]\nval_idx   = [i for i, m in enumerate(meta_list) if m[\"game_id\"] in val_game_ids]\n\nX_train_raw = [X_list[i] for i in train_idx]\ny_train     = [y_list[i] for i in train_idx]\nmeta_train  = [meta_list[i] for i in train_idx]\n\nX_val_raw = [X_list[i] for i in val_idx]\ny_val     = [y_list[i] for i in val_idx]\nmeta_val  = [meta_list[i] for i in val_idx]\n\nmean, std = compute_normalization_stats(X_train_raw)\n\nX_train = apply_normalization(X_train_raw, mean, std)\nX_val   = apply_normalization(X_val_raw,   mean, std)\n\ntrain_ds = HockeySeqDataset(X_train, y_train, meta_train)\nval_ds   = HockeySeqDataset(X_val,   y_val,   meta_val)\n\ntrain_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  collate_fn=collate_pad)\nval_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_pad)\n\ninput_dim = train_ds[0][0].shape[1]\nif use_transformer:\n    model = TransformerOutcomeModel(input_dim=input_dim, d_model=128,\n                                    nhead=4, num_layers=2,  # можно уменьшить слои\n                                    dim_feedforward=256,\n                                    dropout=0.3)\nelse:\n    model = LSTMOutcomeModel(input_dim=input_dim, hidden_dim=128,\n                             num_layers=2, dropout=0.3)\n\nmodel.to(device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer,\n    mode='min',  \n    factor=0.5,  \n    patience=2,  \n    verbose=True\n)\n\n# === Тренировка ===\nfor epoch in range(1, num_epochs + 1):\n    train_loss = train_one_epoch(model, train_loader, optimizer, device)\n    val_loss, val_acc = evaluate(model, val_loader, device)\n    print(f\"Epoch {epoch}: \"\n          f\"train_loss={train_loss:.4f} \"\n          f\"val_loss={val_loss:.4f} \"\n          f\"val_acc={val_acc:.4f}\")\n\n    scheduler.step(val_loss)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T18:59:41.873671Z","iopub.execute_input":"2025-12-11T18:59:41.873892Z","iopub.status.idle":"2025-12-11T19:11:23.976799Z","shell.execute_reply.started":"2025-12-11T18:59:41.873876Z","shell.execute_reply":"2025-12-11T19:11:23.975939Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/1069500635.py:10: DtypeWarning: Columns (49,56) have mixed types. Specify dtype option on import or set low_memory=False.\n  df1 = pd.read_csv(\"/kaggle/input/data-21-24/play_by_play_2021_22.csv\", encoding=\"latin1\")\n/tmp/ipykernel_47/1069500635.py:11: DtypeWarning: Columns (48,49,56) have mixed types. Specify dtype option on import or set low_memory=False.\n  df2 = pd.read_csv(\"/kaggle/input/data-21-24/play_by_play_2022_23.csv\", encoding=\"latin1\")\n/tmp/ipykernel_47/1069500635.py:12: DtypeWarning: Columns (42,43,49,50,120,136) have mixed types. Specify dtype option on import or set low_memory=False.\n  df3 = pd.read_csv(\"/kaggle/input/data-21-24/play_by_play_2023_24.csv\", encoding=\"latin1\")\n","output_type":"stream"},{"name":"stdout","text":"y NaN: 0\ny уникальные значения: [0. 1.]\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:508: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n  output = torch._nested_tensor_from_mask(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: train_loss=0.4924 val_loss=0.4683 val_acc=0.7749\nEpoch 2: train_loss=0.4648 val_loss=0.4737 val_acc=0.7736\nEpoch 3: train_loss=0.4612 val_loss=0.5044 val_acc=0.7656\nEpoch 4: train_loss=0.4570 val_loss=0.4792 val_acc=0.7743\nEpoch 5: train_loss=0.4467 val_loss=0.4683 val_acc=0.7813\nEpoch 6: train_loss=0.4432 val_loss=0.4718 val_acc=0.7828\nEpoch 7: train_loss=0.4405 val_loss=0.4659 val_acc=0.7867\nEpoch 8: train_loss=0.4381 val_loss=0.4656 val_acc=0.7825\nEpoch 9: train_loss=0.4365 val_loss=0.4826 val_acc=0.7762\nEpoch 10: train_loss=0.4356 val_loss=0.4811 val_acc=0.7779\nEpoch 11: train_loss=0.4344 val_loss=0.4826 val_acc=0.7885\nEpoch 12: train_loss=0.4272 val_loss=0.4718 val_acc=0.7881\nEpoch 13: train_loss=0.4247 val_loss=0.4730 val_acc=0.7875\nEpoch 14: train_loss=0.4226 val_loss=0.4722 val_acc=0.7907\nEpoch 15: train_loss=0.4182 val_loss=0.4720 val_acc=0.7905\nEpoch 16: train_loss=0.4165 val_loss=0.4592 val_acc=0.7898\nEpoch 17: train_loss=0.4148 val_loss=0.4632 val_acc=0.7921\nEpoch 18: train_loss=0.4135 val_loss=0.4633 val_acc=0.7905\nEpoch 19: train_loss=0.4129 val_loss=0.4660 val_acc=0.7900\nEpoch 20: train_loss=0.4109 val_loss=0.4663 val_acc=0.7898\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"evaluate_by_minute_and_period(model, val_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-11T19:11:23.978309Z","iopub.execute_input":"2025-12-11T19:11:23.978710Z","iopub.status.idle":"2025-12-11T19:11:27.567229Z","shell.execute_reply.started":"2025-12-11T19:11:23.978687Z","shell.execute_reply":"2025-12-11T19:11:27.566355Z"}},"outputs":[{"name":"stdout","text":"\n=== Period 1 ===\nMinute  5: acc=0.500 (n=2)\nMinute  6: acc=0.737 (n=19)\nMinute  7: acc=0.780 (n=82)\nMinute  8: acc=0.807 (n=212)\nMinute  9: acc=0.835 (n=375)\nMinute 10: acc=0.823 (n=564)\nMinute 11: acc=0.831 (n=692)\nMinute 12: acc=0.845 (n=766)\nMinute 13: acc=0.835 (n=812)\nMinute 14: acc=0.862 (n=825)\nMinute 15: acc=0.877 (n=832)\nMinute 16: acc=0.893 (n=834)\nMinute 17: acc=0.921 (n=836)\nMinute 18: acc=0.946 (n=837)\nMinute 19: acc=0.975 (n=837)\nMinute 20: acc=0.999 (n=837)\n\n=== Period 2 ===\nMinute  1: acc=0.625 (n=837)\nMinute  2: acc=0.651 (n=837)\nMinute  3: acc=0.671 (n=837)\nMinute  4: acc=0.676 (n=837)\nMinute  5: acc=0.681 (n=837)\nMinute  6: acc=0.722 (n=837)\nMinute  7: acc=0.736 (n=837)\nMinute  8: acc=0.760 (n=837)\nMinute  9: acc=0.785 (n=837)\nMinute 10: acc=0.798 (n=837)\nMinute 11: acc=0.816 (n=837)\nMinute 12: acc=0.806 (n=837)\nMinute 13: acc=0.795 (n=837)\nMinute 14: acc=0.800 (n=837)\nMinute 15: acc=0.811 (n=837)\nMinute 16: acc=0.806 (n=837)\nMinute 17: acc=0.811 (n=837)\nMinute 18: acc=0.820 (n=837)\nMinute 19: acc=0.822 (n=837)\nMinute 20: acc=0.816 (n=837)\n\n=== Period 3 ===\nMinute  1: acc=0.706 (n=837)\nMinute  2: acc=0.716 (n=837)\nMinute  3: acc=0.705 (n=837)\nMinute  4: acc=0.723 (n=837)\nMinute  5: acc=0.741 (n=837)\nMinute  6: acc=0.728 (n=837)\nMinute  7: acc=0.756 (n=837)\nMinute  8: acc=0.763 (n=837)\nMinute  9: acc=0.775 (n=837)\nMinute 10: acc=0.780 (n=837)\nMinute 11: acc=0.784 (n=837)\nMinute 12: acc=0.779 (n=837)\nMinute 13: acc=0.768 (n=837)\nMinute 14: acc=0.772 (n=837)\nMinute 15: acc=0.781 (n=837)\nMinute 16: acc=0.784 (n=837)\nMinute 17: acc=0.784 (n=837)\nMinute 18: acc=0.799 (n=837)\nMinute 19: acc=0.806 (n=837)\nMinute 20: acc=0.820 (n=837)\n","output_type":"stream"}],"execution_count":4}]}